---
title: "Clase 2: Aprendizaje Supervisado"
author: 
- Pamela E. Pairo
- Posgrado Digital Accounting
title-slide-attributes:
    data-background-image: "img/portada.jpg"
format: 
  revealjs:
    theme: [mytheme.scss]
    slide-number: c
    incremental: false
    width: 1600
    height: 1000
    logo: "img/logo.jpg"
    footer: "[Aprendizaje Supervisado- PDA](https://github.com/PamelaPairo/intro_ia_pda)"
    chalkboard:
      theme: whiteboard
      boardmarker-width: 5
      buttons: true
    echo: true
editor: visual
---

## En la clase de hoy...

[**En la primera parte:**]{style="color: #ae260e;"}

. . .

-   Casos de uso de ML: Riesgo crediticio y Detecci√≥n de Fraude
-   √Årboles de decisi√≥n
-   Ensambles: Bagging y Boosting
-   Ejercitaci√≥n

. . .

[**Recreo**]{style="color: #ae260e;"} ‚òïüßâ

. . .

[**En la segunda parte:**]{style="color: #ae260e;"}

. . .

-   Series Temporales
-   Ejercitaci√≥n

## Ejemplos de clasificaci√≥n supervisada

Recordemos...

-   [**Aprendizaje supervisado**]{style="color: #88188a;"}: Datos de entrenamiento + salida esperada

. . .

::: columns
::: {.column width="50%"}

![](https://appinventiv.com/wp-content/uploads/2021/12/How-Machine-Learning-Helps-in-Financial-Fraud-Detection-in-the-FinTech-Industry_Info-1-10-2048x1330.webp){.fragment width="950" fig-align="center"}

:::

::: {.column width="50%"}
::: incremental
![](https://www.solulab.com/wp-content/uploads/2024/05/Types-of-Credit-Risk.jpg){.fragment width="950" fig-align="center"}
:::
:::
:::
## Ventajas de usar Machine Learning

::: columns
::: {.column width="60%"}

![](img/ventajas.png){.fragment width="1000" fig-align="center"}

:::

::: {.column width="40%"}
::: incremental

- Identificaci√≥n r√°pida y precisa de comportamientos sospechosos.

- Procesamiento de grandes cantidades de datos en poco tiempo.

- Automatizaci√≥n de tareas repetitivas lo cual reduce tiempo y costos y por ende los procesos son mas eficientes.

:::
:::
:::

##  {background-color="#C5E1A5"}

<h3 style="color:black;background-color: rgba(255,255,255,0.65);padding:5px;line-height:2em; text-align: center; position: absolute; top: 40%; width: 100%;">

[√Årboles de decisi√≥n üå≤]{style="font-size: 100px;"}

</h3>

## √Årboles de decisi√≥n {chalkboard-buttons="true"}

::: columns
::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/arbol.png){.fragment width="850" fig-align="center"}
:::

::: {.column width="50%"}
::: incremental
-   Aprende a diferenciar los datos en base a [**reglas de decisi√≥n**]{style="color: #88188a;"}.

-   Funcionan bien para datos no linealmente separables.

-   Cada nodo interno compara un atributo xi

-   Una rama por cada valor de atributo xi=v

-   Cada hoja asigna una clase y

-   ¬°Modelo interpretable!
:::
:::
:::

## ¬øCon qu√© atributo comenzar?

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/arbol2.png){fig-align="center"}

> Una partici√≥n es buena si estamos "m√°s seguros" de la clasificaci√≥n despu√©s de haberla realizado

## ¬øQu√© atributo es el mejor?

[**Entrop√≠a**]{style="color: #88188a;"}

Medida de incertidumbre de una variable aleatoria

$\LARGE H(Y) = -\sum_{i=1}^{k}p(Y=y_i)log_2 p(Y=y_i)$

. . .

::: {style="text-align: center;"}
> **A mayor incertidumbre, mayor entrop√≠a**
:::

. . .

[**Entrop√≠a Condicional**]{style="color: #88188a;"}

$H(Y|X) = -\sum_{j=1}^{v}p(X=x_j)\sum_{i=1}^{k}p(Y=y_i|X=x_j)log_2 p(Y=y_i| X=x_j)$

## ¬øQu√© atributo es el mejor?

[**Ganancia de Informaci√≥n**]{style="color: #88188a;"}

Decrecimiento de entrop√≠a (incertidumbre) luego de la partici√≥n

::: {style="text-align: center;"}
$\LARGE IG(X)= H(Y)- H(Y|X)$

> [**Elijo el atributo que me de mayor ganancia de informaci√≥n**]{style="font-size: 55px;"}
:::

## Aprendizaje en √Årboles de decisi√≥n

::: columns
::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/iris.png){fig-align="center"}
:::

::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/iris_plot.png){fig-align="center"}
:::
:::

::: incremental
-   Comenzar con un √°rbol vac√≠o

-   Generar una partici√≥n usando siguiente mejor atributo

-   Usar, por ejemplo, ganancia de informaci√≥n y realizar el paso anterior de manera recursiva
:::

## ¬øHasta cuando dejo de "partir" un nodo?

> [**Hiperpar√°metro**]{style="color: #88188a;"}: Valores no aprendidos por el algoritmo desde los datos y por ende deben ser seteados antes de entrenar el algoritmo.

### En √°rboles de decisi√≥n:

`n_min` : n m√≠nimo para dividir los nodos

`tree_depth`: l√≠mite a la profundidad del √°rbol

`cost_complexity`: costo o penalizaci√≥n a los errores de √°rboles m√°s complejos. Es una forma de poda.

## Sobreajuste

-   El error de entrenamiento es siempre cero

-   Poca capacidad de generalizaci√≥n.

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/overfitting.png){fig-align="center"}

> [El objetivo de un modelo de Machine Learning es que generalice bien frente a nuevos datos o datos no vistos por el modelo (set de testeo)]{style="font-size: 55px; padding:15px"}

::: footer
Imagen extra√≠da de [aqu√≠](https://bookdown.org/content/2031/arboles-de-decision-parte-i.html)
:::

## Sesgo y varianza

![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/sesgo.png){fig-align="center"}

El sesgo (o bias) es la diferencia entre el valor medio predicho por el modelo y el valor medio real.

La varianza se puede relacionar con la complejidad de los modelos. A medida que aumenta la complejidad, aumentan las posibilidades de sobreajuste, es decir, la varianza aumenta.

## 

[**Ventajas üí°**]{style="color: #88188a; font-size: 55px"}

-   F√°cil de entender
-   Util en exploraci√≥n de datos:identificar importancia de variables a partir de cientos de variables.
-   El tipo de datos no es una restricci√≥n
-   Es un m√©todo no param√©trico (i.e., no hay suposici√≥n acerca del espacio de distribuci√≥n y la estructura del clasificador)

. . .

[**Desventajas üí°**]{style="color: #88188a; font-size: 55px"}

-   Sobreajuste
-   P√©rdida de informaci√≥n al categorizar variables continuas
-   Inestables debido a que peque√±as variaciones en el dataset pueden generar modelos muy diferentes.

##  {background-color="#C5E1A5"}

<h3 style="color:black;background-color: rgba(255,255,255,0.65);padding:5px;line-height:2em; text-align: center; position: absolute; top: 40%; width: 100%;">

[Ensamble Learning üå≤üå≤üå≤]{style="font-size: 100px;"}

</h3>

## Ensamble learning: Motivaci√≥n

Se basan en la idea de que el trabajo en conjunto deber√≠a dar mejores resultados.

Habitualmente, un modelo "ensemble" es m√°s preciso que los modelos que lo constituyen. Intuitivamente, esto se debe a que "dos aprenden mejor que uno".

. . .

> Con los m√©todos de ensemble se pueden combinar m√∫ltiples modelos en uno nuevo y as√≠ lograr un equilibro entre [**sesgo y varianza**]{style="color: #88188a;"}, y por ende conseguir mejores predicciones que cualquiera de los modelos individuales originales

## Ensemble learning: Bagging

Los [**√°rboles de decisi√≥n**]{style="color: #88188a;"} son algoritmos inestables debido a que peque√±as variaciones en el dataset pueden generar modelos muy diferentes.

. . .

> [**Bagging (Bootstrap Aggregation)**]{style="color: #88188a;"} es un m√©todo para hacer aprendizaje por *ensemble*. Consiste en realizar K subsets del dataset aleatoriamente y con reemplazo, resultando en un ensamble de K modelos. La asignaci√≥n de la clase se realiza por mayoria simple en casos de clasificaci√≥n.

## Ensemble learning: Bagging

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/bagging.png){fig-align="center"}

::: footer
Imagen de Hendrik Blockeel
:::

## Random Forest

-   Es una modificaci√≥n a Bagging para √Årboles de Decisi√≥n.

-   En cada √°rbol se consideran s√≥lo M atributos elegidos aleatoriamente.

-   El algoritmo es sencillo, f√°cil de implementar, f√°cil de usar y requiere de poco ajuste de par√°metros.

-   Es menos interpretable que los √°rboles de decisi√≥n.

![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/random_forest.png){fig-align="center"}

## Ensamble learning: Boosting

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/boosting.png){fig-align="center"}

::: footer
Imagen extra√≠da de [aqu√≠](https://datascience.eu/machine-learning/gradient-boosting-what-you-need-to-know/)
:::

## Bagging vs Boosting

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/bag_vs_boost.jpeg){fig-align="center"}

::: footer
Imagen extra√≠da de [aqu√≠](https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422)
:::

##  {background-image="img/computadora.jpg" background-size="cover"}

<h3 style="color:black;background-color: rgba(255,255,255,0.65);padding:15px;line-height:2em; text-align: center; position: absolute; top: 25%; width: 56%;">

[¬°Manos a R!]{style="font-size: 100px;"}

</h3>

::: footer
Foto de <a href="https://unsplash.com/@emilep?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Emile Perron</a> en <a href="https://unsplash.com/es?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
:::

##  {background-color="#C5E1A5"}

<h3 style="color:black;background-color: rgba(255,255,255,0.65);padding:5px;line-height:2em; text-align: center; position: absolute; top: 40%; width: 100%;">

[Series Temporales]{style="font-size: 100px;"}

</h3>

## ¬øQu√© es una serie temporal?

> Secuencia de N observaciones (datos) ordenados y equidistantes cronologicamente sobre una caracter√≠stica de una unidad observable en diferentes momentos. Es decir la variable x es el tiempo

![](https://support.minitab.com/es-mx/minitab/20/media/generated-content/images/time_scale_default_ticks.png){.fragment width="800" fig-align="center"}

## Algunas aplicaciones

::: incremental
::: columns
::: {.column width="33%"}
**Econom√≠a**

-   Precio de un producto
-   Tasa de desempleo
-   Tasa de inflaci√≥n
-   Precio del d√≥lar
-   Precio de acciones
:::

::: {.column width="34%"}
**Meteorolog√≠a**

-   Temperatura m√°xima diaria
-   Velocidad del viento
-   Precipitaciones
:::

::: {.column width="33%"}
**Marketing**

-   Gastos
-   Ventas
-   Demandas
-   Ofertas
:::
:::
:::

## Tipos de an√°lisis

![](img/st.png){.fragment width="80" fig-align="center"}

::: incremental
-   Descriptivo: ¬øC√∫ando hay baja/alta demanda de un producto?
-   Explicativo/Inferencial: ¬øA qu√© se debe que el producto haya aumentado/decrecido sus ventas?
-   Predictivo (*Forecasting*): Se usa el comportamiento hist√≥rico de la serie para conocer el comportamiento futuro
:::

> Importante el conocimiento de dominio, plantear adecuadamente el objetivo y as√≠ decidir la periodicidad de la serie

## Componentes de la serie temporal üìà

**Tendencia (*Trend*):** existe un aumento o disminuci√≥n a largo plazo en los datos. No tiene por qu√© ser lineal.

![](img/trend.png){fig-align="center"}

## Componentes de la serie temporal

**Estacionalidad (*Seasonality*):** ocurre cuando una serie de tiempo se ve afectada por factores estacionales como la √©poca del a√±o o el d√≠a de la semana. La estacionalidad es siempre de un per√≠odo fijo y conocido.

![](img/seas_trend.png){fig-align="center"}

## Componentes de la serie temporal

**C√≠clico:** ocurre cuando la exhibici√≥n de datos sube y baja pero no con una frecuencia fija.

![](img/cycle.png){fig-align="center"}

> Si las fluctuaciones no son de una frecuencia fija, entonces son c√≠clicas; si la frecuencia no cambia y est√° asociada con alg√∫n aspecto del calendario, entonces el patr√≥n es estacional.

## Descomposici√≥n: Aditiva vs Multiplicativa

-   Aditivo: es lineal donde los cambios a lo largo del tiempo se realizan consistentemente en la misma cantidad. Una tendencia lineal es una l√≠nea recta. Una estacionalidad lineal tiene la misma frecuencia (ancho de ciclos) y amplitud (alto de ciclos).

-   Multiplicativo: es no lineal, como cuadr√°tico o exponencial. Los cambios aumentan o disminuyen con el tiempo. Una tendencia no lineal es una l√≠nea curva. Una estacionalidad no lineal tiene una frecuencia y/o amplitud creciente o decreciente a lo largo del tiempo.

![](img/admul.png){fig-align="center"}

## Modelos Autoregresivos - AR(p)

Se pronostica la variable de inter√©s utilizando una combinaci√≥n lineal de valores pasados de la variable.

El t√©rmino autoregresivo describe la regresi√≥n de una variable consigo misma.

$\LARGE y_t= c + \phi_1 * y_{t-1} + \phi_2 * y_{t-2} + ...\phi_p * y_{t-p} + \epsilon_t$

$\phi_p$ = coeficientes de retardo

$\epsilon_t$ = white noise

## Modelos de media m√≥vil - MA (q)

En lugar de usar valores pasados de la variable de pron√≥stico en una regresi√≥n, un modelo de promedio m√≥vil usa errores de pron√≥stico pasados en un modelo similar a una regresi√≥n.

$\LARGE y_t= c + \epsilon_t + \theta_1 * \epsilon_{t-1} + ...\theta_q * \epsilon_{t-q}$

## Diferenciaci√≥n (I)

Una serie es estacionaria cuando la media y la varianza son constantes a lo largo del tiempo, **es decir no tiene componente de tendencia.**

> Si la serie no es estacionaria se debe indicar cuantas veces hay que diferenciarla para que los modelos AR y MA sean confiables

## Autocorrelation Function y Partial Autocorrelation Function

::: columns
::: {.column width="50%"}
ACF: Autocorrelaci√©n entre la serie y una retrasada en si misma que incluye la dependecnia directa e indirecta de la informaci√≥n

PACF: solo describe la relaci√≥n directa entre la observaci√≥n y la retrasada en el tiempo
:::

::: {.column width="50%"}
![](img/acf_pacf.png){fig-align="center"}
:::
:::

## ARIMA y SARIMA

![](img/sarima.png){.fragment width="60" fig-align="center"}

## Exponential Smoothing

Los pron√≥sticos producidos son promedios ponderados de observaciones pasadas, con los pesos decayendo exponencialmente a medida que las observaciones envejecen. En otras palabras, cuanto m√°s reciente es la observaci√≥n, mayor es el peso asociado.

Exponencial vs ARIMA: Mientras que los modelos de suavizado exponencial se basan en una descripci√≥n de la tendencia y la estacionalidad de los datos, los modelos ARIMA tienen como objetivo describir las autocorrelaciones en los datos.

## Exponential Smoothing

**Suavizado simple**: este m√©todo es adecuado para pronosticar datos sin tendencia clara o patr√≥n estacional. Los pron√≥sticos se calculan utilizando promedios ponderados, donde los pesos disminuyen exponencialmente a medida que las observaciones provienen de m√°s lejos en el pasado.

**Suavizado doble**: este m√©todo se utiliza cuando los datos muestran una tendencia constante (creciente o decreciente) indefinidamente en el futuro.

**Suavizado triple-M√©todo Holt Winter**: este m√©todo comprende la ecuaci√≥n de pron√≥stico y tres ecuaciones de suavizado: una para el nivel, una para la tendencia y otra para el componente estacional, con los correspondientes par√°metros de suavizado.

##  {background-image="img/computadora.jpg" background-size="cover"}

<h3 style="color:black;background-color: rgba(255,255,255,0.65);padding:15px;line-height:2em; text-align: center; position: absolute; top: 25%; width: 56%;">

[¬°Manos a R!]{style="font-size: 100px;"}

</h3>

::: footer
Foto de <a href="https://unsplash.com/@emilep?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Emile Perron</a> en <a href="https://unsplash.com/es?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
:::

## Referencias

-   [Introducci√≥n a las Series Temporales](https://www.ucm.es/data/cont/docs/518-2013-11-11-JAM-IAST-Libro.pdf)

-   [Forecasting: Principles and Practice](https://otexts.com/fpp2/)

-   [Applied Machine Learning](https://rstudio-conf-2020.github.io/applied-ml/Part_6.html#1) dictado en rstudio::conf 2020

-   [Data preprocessing and resampling using tidymodels](https://www.youtube.com/watch?v=s3TkvZM60iU&ab_channel=JuliaSilge), tutorial en Youtube de Julia Silge
